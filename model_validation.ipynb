{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"final_cleaned_5W_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "173     0.0\n",
       "174     0.0\n",
       "175     0.0\n",
       "176     2.0\n",
       "177    12.0\n",
       "Name: work_experience_duration, Length: 178, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['work_experience_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns that end with the specified suffixes  \n",
    "suffixes = ['_tech_coding_attempt', '_tech_coding_submitted',   \n",
    "            '_tech_DSA_submitted', '_tech_DSA_attempt',  \n",
    "            '_non-tech_attempt', '_non-tech_submitted']  \n",
    "\n",
    "# Find columns that end with any of these suffixes  \n",
    "columns_to_multiply = []  \n",
    "for suffix in suffixes:  \n",
    "    columns_to_multiply.extend([col for col in df.columns if col.endswith(suffix)])  \n",
    "\n",
    "# Multiply selected columns by 100  \n",
    "for column in columns_to_multiply:  \n",
    "    df[column] = df[column] * 100  \n",
    "\n",
    "# Display the first few rows to verify the changes  \n",
    "# print(merged_final_cleaned[columns_to_multiply].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest_qualification_grouped\n",
      "Engineering    97\n",
      "Science        26\n",
      "Commerce       22\n",
      "Others         17\n",
      "Arts           16\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishit\\AppData\\Local\\Temp\\ipykernel_25180\\2745152757.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['highest_qualification_grouped'] = df['highest_qualification'].apply(group_highest_qualification)\n"
     ]
    }
   ],
   "source": [
    "def group_highest_qualification(qualification):  \n",
    "    if pd.isna(qualification):  \n",
    "        return 'Others'  \n",
    "    \n",
    "    # Convert to string, lowercase, and remove spaces  \n",
    "    qual = str(qualification).lower().strip().replace(' ', '')  \n",
    "    \n",
    "    # Engineering Group  \n",
    "    engineering = [  \n",
    "        'b.tech', 'btech', 'b.e', 'be', 'b.tech/b.e',   \n",
    "        'diploma', 'b.tech(electronicsandcommunication)',  \n",
    "        'mastersofinformationtechnology', 'm.tech'  \n",
    "    ]  \n",
    "    \n",
    "    # Science Group  \n",
    "    science = [  \n",
    "        'b.sc', 'bsc', 'm.sc', 'msc', 'b.pharm', 'b.pharmacy',  \n",
    "        'iti', 'itiwith12th'  \n",
    "    ]  \n",
    "    \n",
    "    # Commerce Group  \n",
    "    commerce = [  \n",
    "        'b.com', 'bcom', 'm.com', 'mcom', 'mba', 'bba', 'mms',  \n",
    "        'bms', 'b.m.s', 'bbi', 'pgdba', 'postgraduatediplomainmanagement',  \n",
    "        'bachelorinhotelmanagement', 'b.a.hospitality'  \n",
    "    ]  \n",
    "    \n",
    "    # Arts Group  \n",
    "    arts = [  \n",
    "        'b.a', 'b.ed', 'm.a', 'b.ed(bachelorofeducation)', 'msw',  \n",
    "        'l.l.b', 'bfa', 'b.arch'  \n",
    "    ]  \n",
    "    \n",
    "    # Computer Applications (could be grouped with Engineering)  \n",
    "    computer_applications = [  \n",
    "        'bca', 'mca', 'pgdca'  \n",
    "    ]  \n",
    "    \n",
    "    # Convert input to comparable format  \n",
    "    qual = qual.replace('.', '').replace('(', '').replace(')', '')  \n",
    "    \n",
    "    if any(q.replace('.', '') in qual for q in engineering + computer_applications):  \n",
    "        return 'Engineering'  \n",
    "    elif any(q.replace('.', '') in qual for q in science):  \n",
    "        return 'Science'  \n",
    "    elif any(q.replace('.', '') in qual for q in commerce):  \n",
    "        return 'Commerce'  \n",
    "    elif any(q.replace('.', '') in qual for q in arts):  \n",
    "        return 'Arts'  \n",
    "    else:  \n",
    "        return 'Others'  \n",
    "\n",
    "# Apply the grouping  \n",
    "df['highest_qualification_grouped'] = df['highest_qualification'].apply(group_highest_qualification)  \n",
    "\n",
    "# To verify the grouping  \n",
    "print(df['highest_qualification_grouped'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Engineering\n",
       "1      Engineering\n",
       "2      Engineering\n",
       "3      Engineering\n",
       "4      Engineering\n",
       "          ...     \n",
       "173           Arts\n",
       "174    Engineering\n",
       "175    Engineering\n",
       "176    Engineering\n",
       "177    Engineering\n",
       "Name: highest_qualification_grouped, Length: 178, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['highest_qualification_grouped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishit\\AppData\\Local\\Temp\\ipykernel_25180\\3151920123.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['qualification_specialisation_grouped'] = df['qualification_sepcialisation'].apply(group_qualifications)\n"
     ]
    }
   ],
   "source": [
    "def group_qualifications(specialization):  \n",
    "    if pd.isna(specialization):  \n",
    "        return 'Others'  \n",
    "    \n",
    "    # Convert input to lowercase and strip spaces for comparison  \n",
    "    specialization = str(specialization).lower().strip().replace(' ', '')  \n",
    "    \n",
    "    # Engineering Group  \n",
    "    engineering = [x.lower().strip().replace(' ', '') for x in [  \n",
    "        'CS', 'Mechanical', 'Computers', 'SoftwareDevelopment', 'ECE',  \n",
    "        'ComputerApplications', 'IT', 'ComputerEngineering', 'Civil',  \n",
    "        'Electrical', 'Electrical&Electronics', 'Electronics',  \n",
    "        'ElectronicsEngineering', 'ProductionEngineering', 'Automobile',  \n",
    "        'Metallurgy', 'ChemicalEngineering', 'Electronics&Instrumentation',  \n",
    "        'Mechatronics', 'EnergySystemsEngineer', 'Mehcnnanical', 'Aeronautical',  \n",
    "        'TextileTechnology', 'PlasticEngineering', 'CyberSecurity', 'CSE',  \n",
    "        'InstrumentationandControl', 'IndustrialandProduction',  \n",
    "        'AgricultureEngineering', 'ApparelTechnology', 'Architecture',  \n",
    "        'Manufacturing', 'AutomobileEngineering', 'FoodEngineering',  \n",
    "        'ManufacturingEngineering', 'EEE', 'TelecommunicationEngineering',  \n",
    "        'Bioengineering', 'MineralEngineering', 'MetallurgicalandMaterialsEngineering',  \n",
    "        'Petroleumengineering', 'MedicalElectronics', 'AgriculturalEngineering',  \n",
    "        'Cybersecurity'  \n",
    "    ]]  \n",
    "    \n",
    "    # Science Group  \n",
    "    science = [x.lower().strip().replace(' ', '') for x in [  \n",
    "        'Biology', 'PharmaceuticalChemistry', 'Chemistry', 'Physics', 'Zoology',  \n",
    "        'ForensicScience', 'Science', 'Biotechnology', 'FoodTechnology',  \n",
    "        'Biochemistry', 'BZC', 'Physics(Hons)', 'Nutraceuticals',  \n",
    "        'ScienceBackgroundstudentin12thStandard', 'Botany',  \n",
    "        'ElectronicScience', 'Materials', 'IndustrialChemistry', 'Pharmacy',  \n",
    "        'Nanotechnology', 'Chemistry'  \n",
    "    ]]  \n",
    "    \n",
    "    # Mathematics Group  \n",
    "    mathematics = [x.lower().strip().replace(' ', '') for x in [  \n",
    "        'MathematicsWithComputerApplication', 'Mathematics', 'PCM',  \n",
    "        'MathandScience', 'Statistics', 'Mathmatics', 'Statistic',  \n",
    "        'Physics,Statistics,Mathematics', 'Mathematics', 'B.ScMathematics'  \n",
    "    ]]  \n",
    "    \n",
    "    # Commerce Group  \n",
    "    commerce = [x.lower().strip().replace(' ', '') for x in [  \n",
    "        'Accounts', 'Accounts&Finance', 'Finance', 'Management', 'BBA',  \n",
    "        'Accounting', 'Marketing', 'BusinessManagement', 'Marketing&Operations',  \n",
    "        'Corporation', 'Finance&HR', 'OperationsandLogisticsManagement',  \n",
    "        'Marketing&Finance', 'BusinessAnalytics', 'Business',  \n",
    "        'HumanResource', 'DigitalMarketing', 'HumanResourceandMarketingManagement',  \n",
    "        'HR&IT', 'Banking', 'OperationsResearch', 'CommercialApplication', 'Commerce',  \n",
    "        'Economics'  \n",
    "    ]]  \n",
    "    \n",
    "    # Arts Group  \n",
    "    arts = [x.lower().strip().replace(' ', '') for x in [  \n",
    "        'Bengali', 'English', 'Geography', 'History', 'Philosophy', 'Arts',  \n",
    "        'PoliticalScience', 'Language', 'Sociology', 'Education', 'Journalism',  \n",
    "        'Finearts', 'FashionDesign'  \n",
    "    ]]  \n",
    "    \n",
    "    if specialization in engineering:  \n",
    "        return 'Engineering'  \n",
    "    elif specialization in science:  \n",
    "        return 'Science'  \n",
    "    elif specialization in mathematics:  \n",
    "        return 'Mathematics'  \n",
    "    elif specialization in commerce:  \n",
    "        return 'Commerce'  \n",
    "    elif specialization in arts:  \n",
    "        return 'Arts'  \n",
    "    else:  \n",
    "        return 'Others'  \n",
    "\n",
    "# Create new column  \n",
    "df['qualification_specialisation_grouped'] = df['qualification_sepcialisation'].apply(group_qualifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishit\\AppData\\Local\\Temp\\ipykernel_25180\\3820904840.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cleaned_state'] = df['current_state'].apply(clean_state_name)\n",
      "C:\\Users\\Rishit\\AppData\\Local\\Temp\\ipykernel_25180\\3820904840.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['state_grouped'] = df['current_state'].apply(group_states)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your original DataFrame\n",
    "# First, group by state and count\n",
    "state_counts = df['current_state'].value_counts()\n",
    "\n",
    "# Calculate percentages\n",
    "state_percentages = state_counts / len(df) * 100\n",
    "\n",
    "# Sort in descending order\n",
    "sorted_percentages = state_percentages.sort_values(ascending=False)\n",
    "\n",
    "# Calculate cumulative percentage\n",
    "cumulative_percentages = sorted_percentages.cumsum()\n",
    "\n",
    "# Find states that make up 80% of the data\n",
    "states_to_keep = cumulative_percentages[cumulative_percentages <= 80]\n",
    "\n",
    "# Create a mapping for state names (optional: clean up state names)\n",
    "def clean_state_name(state):\n",
    "    # Convert to title case and strip whitespace\n",
    "    cleaned = str(state).title().strip()\n",
    "    # Handle specific cases\n",
    "    replacements = {\n",
    "        'Jammu & Kashmir': 'Jammu and Kashmir',\n",
    "        'J&K': 'Jammu and Kashmir',\n",
    "        'Ut': 'Uttar Pradesh',\n",
    "        'Up': 'Uttar Pradesh',\n",
    "        'Orissa': 'Odisha'\n",
    "    }\n",
    "    return replacements.get(cleaned, cleaned)\n",
    "\n",
    "# Clean state names\n",
    "df['cleaned_state'] = df['current_state'].apply(clean_state_name)\n",
    "\n",
    "# Define the list of states to keep  \n",
    "main_states = [  \n",
    "    'Maharashtra',   \n",
    "    'Uttar Pradesh',   \n",
    "    'Karnataka',   \n",
    "    'Delhi',   \n",
    "    'Madhya Pradesh',   \n",
    "    'Bihar',   \n",
    "    'West Bengal',   \n",
    "    'Haryana',   \n",
    "    'Rajasthan',   \n",
    "    'Jharkhand',   \n",
    "    'Andhra Pradesh'  \n",
    "]  \n",
    "\n",
    "# Create a function to group states  \n",
    "def group_states(state):  \n",
    "    # Normalize state names (remove extra spaces, handle variations)  \n",
    "    state = str(state).strip().title()  \n",
    "    \n",
    "    # Special handling for variations  \n",
    "    state_mappings = {  \n",
    "        'Uttar\\xa0Pradesh': 'Uttar Pradesh',  \n",
    "        'Uttar Pradesh': 'Uttar Pradesh',  \n",
    "        'Karnatka': 'Karnataka',  \n",
    "        'Karanataka': 'Karnataka'  \n",
    "    }  \n",
    "    \n",
    "    state = state_mappings.get(state, state)  \n",
    "    \n",
    "    # Return state if in main list, otherwise 'Other'  \n",
    "    return state if state in main_states else 'Other'  \n",
    "\n",
    "# Apply the grouping  \n",
    "df['state_grouped'] = df['current_state'].apply(group_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_grouped\n",
       "Maharashtra       33\n",
       "Other             33\n",
       "Madhya Pradesh    16\n",
       "Bihar             16\n",
       "Uttar Pradesh     16\n",
       "Karnataka         16\n",
       "West Bengal       14\n",
       "Delhi             12\n",
       "Andhra Pradesh     6\n",
       "Haryana            6\n",
       "Jharkhand          5\n",
       "Rajasthan          5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['state_grouped'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Engineering', 'Others', 'Commerce', 'Science', 'Arts']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df['highest_qualification_grouped'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'section_name_x':'section_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Engineering\n",
       "1      Engineering\n",
       "2      Engineering\n",
       "3      Engineering\n",
       "4      Engineering\n",
       "          ...     \n",
       "173           Arts\n",
       "174    Engineering\n",
       "175    Engineering\n",
       "176    Engineering\n",
       "177    Engineering\n",
       "Name: highest_qualification_grouped, Length: 178, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['highest_qualification_grouped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['id',\n",
    " 'user_id',\n",
    " 'section_id',\n",
    " 'date_of_clearance',\n",
    " 'user_code',\n",
    " 'course',\n",
    " 'placement_status',\n",
    " 'tech_stack',\n",
    " 'name',\n",
    " 'gender',\n",
    " 'subrole',\n",
    " 'current_state',\n",
    " 'current_city',\n",
    " 'highest_qualification',\n",
    " 'qualification_sepcialisation',\n",
    " 'tenth_percentage',\n",
    " 'twelfth_percentage',\n",
    " 'grad_year',\n",
    " 'grad_percentage',\n",
    " 'work_experience_any',\n",
    " 'work_experience_relevant',\n",
    " 'work_experience_duration',\n",
    " 'current_ctc',\n",
    " 'last_company',\n",
    " 'cohort_reco',\n",
    " 'days_in_placement',\n",
    " 'position_id',\n",
    " 'company_name',\n",
    " 'ctc',\n",
    " 'isa_status',\n",
    " 'morat_probation_internship_period',\n",
    " 'morat_salary',\n",
    " 'verbal_offer_date',\n",
    " 'offer_letter_date',\n",
    " 'offer_letter_sent_to_ops',\n",
    " 'cc',\n",
    " 'bd_poc',\n",
    " 'hired_post',\n",
    " 'removed_from_placement_workspace',\n",
    " 'experience_form_filled',\n",
    " 'hired_before',\n",
    " 'disbursal_date',\n",
    " 'emi_repayment_start_date',\n",
    " 'nbfc',\n",
    " 'amount',\n",
    " 'is_active',\n",
    " 'created_at',\n",
    " 'updated_at',\n",
    " 'placement_sub_status',\n",
    " 'placement_history_id',\n",
    " 'disbursal_status',\n",
    " 'disbursal_status_last_updated_at',\n",
    " 'role',\n",
    " 'pod_bd_poc',\n",
    " 'pod_cc',\n",
    " 'pod_cl',\n",
    " 'pod_curriculum_poc',\n",
    " 'pod_csbt_poc',\n",
    " 'pod_ec',\n",
    " 'pod_poc_lead',\n",
    " 'date_of_joining',\n",
    " 'pod',\n",
    " 'cfrs',\n",
    " 'workingmode',\n",
    " 'position_type',\n",
    " 'level_up_email',\n",
    " 'level_up_email_type',\n",
    " 'slack_account_status',\n",
    " 'slack_email',\n",
    " 'lu_javascript_skillscore',\n",
    " 'lu_react_skillscore',\n",
    " 'lu_dsa_skillscore',\n",
    " 'lu_mongodb_skillscore',\n",
    " 'lu_sql_skillscore',\n",
    " 'lu_manual_testing_skillscore',\n",
    " 'lu_selenium_skillscore',\n",
    " 'lu_core_java_skillscore',\n",
    " 'lu_spring_skillscore',\n",
    " 'lu_node_express_skillscore',\n",
    " 'lu_html_css_skillscore',\n",
    " 'assess_excel_skillscore',\n",
    " 'assess_javascript_skillscore',\n",
    " 'assess_mongo_skillscore',\n",
    " 'assess_nodeexpress_skillscore',\n",
    " 'assess_powerbi_skillscore',\n",
    " 'assess_python_skillscore',\n",
    " 'assess_react_skillscore',\n",
    " 'assess_sql_skillscore',\n",
    " 'assess_interview_score',\n",
    " 'opportunity_type',\n",
    " 'currently_studying',\n",
    " 'currently_working',\n",
    " 'notice_period',\n",
    " 'current_or_last_ctc',\n",
    " 'id_y',\n",
    " 'mobile',\n",
    " 'email',\n",
    " 'mock_test_link',\n",
    " 'marks',\n",
    " 'percentile',\n",
    " 'updated_at_email',\n",
    " 'cohort_id',\n",
    " 'Profile_slug',\n",
    " 'Profile_email',\n",
    " 'Profile_number',\n",
    " 'updated_at_mobile',\n",
    " 'user_code_z',\n",
    " 'name_y',\n",
    " 'email_y',\n",
    " 'status',\n",
    " 'section_id_y',\n",
    " 'section_name',\n",
    " 'level',\n",
    " 'course_type',\n",
    " 'last_active_since',\n",
    " 'batch_id',\n",
    " 'batch_name',\n",
    " 'block_name',\n",
    " 'p1_tech_coding',\n",
    " 'p1_tech_DSA',\n",
    " 'p1_non-tech',\n",
    " 'p2_tech_coding',\n",
    " 'p2_tech_DSA',\n",
    " 'p2_non-tech',\n",
    " 'p3_tech_coding',\n",
    " 'p3_tech_DSA',\n",
    " 'p3_non-tech',\n",
    " 'p4_tech_coding',\n",
    " 'p4_tech_DSA',\n",
    " 'p4_non-tech',\n",
    " 'p5_tech_coding',\n",
    " 'p5_tech_DSA',\n",
    " 'p5_non-tech',\n",
    " 'p6_tech_coding',\n",
    " 'p6_tech_DSA',\n",
    " 'p6_non-tech',\n",
    " 'p7_tech_coding',\n",
    " 'p7_tech_DSA',\n",
    " 'p7_non-tech',\n",
    " 'p8_tech_coding',\n",
    " 'p8_tech_DSA',\n",
    " 'p8_non-tech',\n",
    " 'p9_tech_coding',\n",
    " 'p9_tech_DSA',\n",
    " 'p9_non-tech',\n",
    " 'p10_tech_coding',\n",
    " 'p10_tech_DSA',\n",
    " 'p10_non-tech',\n",
    " 'p11_tech_coding',\n",
    " 'p11_tech_DSA',\n",
    " 'p11_non-tech',\n",
    " 'p12_tech_coding',\n",
    " 'p12_tech_DSA',\n",
    " 'p12_non-tech',\n",
    " 'p13_tech_coding',\n",
    " 'p13_tech_DSA',\n",
    " 'p13_non-tech',\n",
    " 'p14_tech_coding',\n",
    " 'p14_tech_DSA',\n",
    " 'p14_non-tech',\n",
    " 'p15_tech_coding',\n",
    " 'p15_tech_DSA',\n",
    " 'p15_non-tech',\n",
    " 'p16_tech_coding',\n",
    " 'p16_tech_DSA',\n",
    " 'p16_non-tech',\n",
    " 'p17_tech_coding',\n",
    " 'p17_tech_DSA',\n",
    " 'p17_non-tech',\n",
    " 'p18_tech_coding',\n",
    " 'p18_tech_DSA',\n",
    " 'p18_non-tech',\n",
    " 'p19_tech_coding',\n",
    " 'p19_tech_DSA',\n",
    " 'p19_non-tech',\n",
    " 'p20_tech_coding',\n",
    " 'p20_tech_DSA',\n",
    " 'p20_non-tech',\n",
    " 'p21_tech_coding',\n",
    " 'p21_tech_DSA',\n",
    " 'p21_non-tech',\n",
    " 'p22_tech_coding',\n",
    " 'p22_tech_DSA',\n",
    " 'p22_non-tech',\n",
    " 'p23_tech_coding',\n",
    " 'p23_tech_DSA',\n",
    " 'p23_non-tech',\n",
    " 'p24_tech_coding',\n",
    " 'p24_tech_DSA',\n",
    " 'p24_non-tech',\n",
    " 'p25_tech_coding',\n",
    " 'p25_tech_DSA',\n",
    " 'p25_non-tech',\n",
    " 'p26_tech_coding',\n",
    " 'p26_tech_DSA',\n",
    " 'p26_non-tech',\n",
    " 'p27_tech_coding',\n",
    " 'p27_tech_DSA',\n",
    " 'p27_non-tech',\n",
    " 'p28_tech_coding',\n",
    " 'p28_tech_DSA',\n",
    " 'p28_non-tech',\n",
    " 'p29_tech_coding',\n",
    " 'p29_tech_DSA',\n",
    " 'p29_non-tech',\n",
    " 'p30_tech_coding',\n",
    " 'p30_tech_DSA',\n",
    " 'p30_non-tech',\n",
    " 'p31_tech_coding',\n",
    " 'p31_tech_DSA',\n",
    " 'p31_non-tech',\n",
    " 'p32_tech_coding',\n",
    " 'p32_tech_DSA',\n",
    " 'p32_non-tech',\n",
    " 'p1_tech_coding_attempt',\n",
    " 'p1_tech_coding_submitted',\n",
    " 'p1_tech_DSA_attempt',\n",
    " 'p1_tech_DSA_submitted',\n",
    " 'p1_non-tech_attempt',\n",
    " 'p1_non-tech_submitted',\n",
    " 'p2_tech_coding_attempt',\n",
    " 'p2_tech_coding_submitted',\n",
    " 'p2_tech_DSA_attempt',\n",
    " 'p2_tech_DSA_submitted',\n",
    " 'p2_non-tech_attempt',\n",
    " 'p2_non-tech_submitted',\n",
    " 'p3_tech_coding_attempt',\n",
    " 'p3_tech_coding_submitted',\n",
    " 'p3_tech_DSA_attempt',\n",
    " 'p3_tech_DSA_submitted',\n",
    " 'p3_non-tech_attempt',\n",
    " 'p3_non-tech_submitted',\n",
    " 'p4_tech_coding_attempt',\n",
    " 'p4_tech_coding_submitted',\n",
    " 'p4_tech_DSA_attempt',\n",
    " 'p4_tech_DSA_submitted',\n",
    " 'p4_non-tech_attempt',\n",
    " 'p4_non-tech_submitted',\n",
    " 'p5_tech_coding_attempt',\n",
    " 'p5_tech_coding_submitted',\n",
    " 'p5_tech_DSA_attempt',\n",
    " 'p5_tech_DSA_submitted',\n",
    " 'p5_non-tech_attempt',\n",
    " 'p5_non-tech_submitted',\n",
    " 'p6_tech_coding_attempt',\n",
    " 'p6_tech_coding_submitted',\n",
    " 'p6_tech_DSA_attempt',\n",
    " 'p6_tech_DSA_submitted',\n",
    " 'p6_non-tech_attempt',\n",
    " 'p6_non-tech_submitted',\n",
    " 'p7_tech_coding_attempt',\n",
    " 'p7_tech_coding_submitted',\n",
    " 'p7_tech_DSA_attempt',\n",
    " 'p7_tech_DSA_submitted',\n",
    " 'p7_non-tech_attempt',\n",
    " 'p7_non-tech_submitted',\n",
    " 'p8_tech_coding_attempt',\n",
    " 'p8_tech_coding_submitted',\n",
    " 'p8_tech_DSA_attempt',\n",
    " 'p8_tech_DSA_submitted',\n",
    " 'p8_non-tech_attempt',\n",
    " 'p8_non-tech_submitted',\n",
    " 'p9_tech_coding_attempt',\n",
    " 'p9_tech_coding_submitted',\n",
    " 'p9_tech_DSA_attempt',\n",
    " 'p9_tech_DSA_submitted',\n",
    " 'p9_non-tech_attempt',\n",
    " 'p9_non-tech_submitted',\n",
    " 'p10_tech_coding_attempt',\n",
    " 'p10_tech_coding_submitted',\n",
    " 'p10_tech_DSA_attempt',\n",
    " 'p10_tech_DSA_submitted',\n",
    " 'p10_non-tech_attempt',\n",
    " 'p10_non-tech_submitted',\n",
    " 'p11_tech_coding_attempt',\n",
    " 'p11_tech_coding_submitted',\n",
    " 'p11_tech_DSA_attempt',\n",
    " 'p11_tech_DSA_submitted',\n",
    " 'p11_non-tech_attempt',\n",
    " 'p11_non-tech_submitted',\n",
    " 'p12_tech_coding_attempt',\n",
    " 'p12_tech_coding_submitted',\n",
    " 'p12_tech_DSA_attempt',\n",
    " 'p12_tech_DSA_submitted',\n",
    " 'p12_non-tech_attempt',\n",
    " 'p12_non-tech_submitted',\n",
    " 'p13_tech_coding_attempt',\n",
    " 'p13_tech_coding_submitted',\n",
    " 'p13_tech_DSA_attempt',\n",
    " 'p13_tech_DSA_submitted',\n",
    " 'p13_non-tech_attempt',\n",
    " 'p13_non-tech_submitted',\n",
    " 'p14_tech_coding_attempt',\n",
    " 'p14_tech_coding_submitted',\n",
    " 'p14_tech_DSA_attempt',\n",
    " 'p14_tech_DSA_submitted',\n",
    " 'p14_non-tech_attempt',\n",
    " 'p14_non-tech_submitted',\n",
    " 'p15_tech_coding_attempt',\n",
    " 'p15_tech_coding_submitted',\n",
    " 'p15_tech_DSA_attempt',\n",
    " 'p15_tech_DSA_submitted',\n",
    " 'p15_non-tech_attempt',\n",
    " 'p15_non-tech_submitted',\n",
    " 'p16_tech_coding_attempt',\n",
    " 'p16_tech_coding_submitted',\n",
    " 'p16_tech_DSA_attempt',\n",
    " 'p16_tech_DSA_submitted',\n",
    " 'p16_non-tech_attempt',\n",
    " 'p16_non-tech_submitted',\n",
    " 'p17_tech_coding_attempt',\n",
    " 'p17_tech_coding_submitted',\n",
    " 'p17_tech_DSA_attempt',\n",
    " 'p17_tech_DSA_submitted',\n",
    " 'p17_non-tech_attempt',\n",
    " 'p17_non-tech_submitted',\n",
    " 'p18_tech_coding_attempt',\n",
    " 'p18_tech_coding_submitted',\n",
    " 'p18_tech_DSA_attempt',\n",
    " 'p18_tech_DSA_submitted',\n",
    " 'p18_non-tech_attempt',\n",
    " 'p18_non-tech_submitted',\n",
    " 'p19_tech_coding_attempt',\n",
    " 'p19_tech_coding_submitted',\n",
    " 'p19_tech_DSA_attempt',\n",
    " 'p19_tech_DSA_submitted',\n",
    " 'p19_non-tech_attempt',\n",
    " 'p19_non-tech_submitted',\n",
    " 'p20_tech_coding_attempt',\n",
    " 'p20_tech_coding_submitted',\n",
    " 'p20_tech_DSA_attempt',\n",
    " 'p20_tech_DSA_submitted',\n",
    " 'p20_non-tech_attempt',\n",
    " 'p20_non-tech_submitted',\n",
    " 'p21_tech_coding_attempt',\n",
    " 'p21_tech_coding_submitted',\n",
    " 'p21_tech_DSA_attempt',\n",
    " 'p21_tech_DSA_submitted',\n",
    " 'p21_non-tech_attempt',\n",
    " 'p21_non-tech_submitted',\n",
    " 'p22_tech_coding_attempt',\n",
    " 'p22_tech_coding_submitted',\n",
    " 'p22_tech_DSA_attempt',\n",
    " 'p22_tech_DSA_submitted',\n",
    " 'p22_non-tech_attempt',\n",
    " 'p22_non-tech_submitted',\n",
    " 'p23_tech_coding_attempt',\n",
    " 'p23_tech_coding_submitted',\n",
    " 'p23_tech_DSA_attempt',\n",
    " 'p23_tech_DSA_submitted',\n",
    " 'p23_non-tech_attempt',\n",
    " 'p23_non-tech_submitted',\n",
    " 'p24_tech_coding_attempt',\n",
    " 'p24_tech_coding_submitted',\n",
    " 'p24_tech_DSA_attempt',\n",
    " 'p24_tech_DSA_submitted',\n",
    " 'p24_non-tech_attempt',\n",
    " 'p24_non-tech_submitted',\n",
    " 'p25_tech_coding_attempt',\n",
    " 'p25_tech_coding_submitted',\n",
    " 'p25_tech_DSA_attempt',\n",
    " 'p25_tech_DSA_submitted',\n",
    " 'p25_non-tech_attempt',\n",
    " 'p25_non-tech_submitted',\n",
    " 'p26_tech_coding_attempt',\n",
    " 'p26_tech_coding_submitted',\n",
    " 'p26_tech_DSA_attempt',\n",
    " 'p26_tech_DSA_submitted',\n",
    " 'p26_non-tech_attempt',\n",
    " 'p26_non-tech_submitted',\n",
    " 'p27_tech_coding_attempt',\n",
    " 'p27_tech_coding_submitted',\n",
    " 'p27_tech_DSA_attempt',\n",
    " 'p27_tech_DSA_submitted',\n",
    " 'p27_non-tech_attempt',\n",
    " 'p27_non-tech_submitted',\n",
    " 'p28_tech_coding_attempt',\n",
    " 'p28_tech_coding_submitted',\n",
    " 'p28_tech_DSA_attempt',\n",
    " 'p28_tech_DSA_submitted',\n",
    " 'p28_non-tech_attempt',\n",
    " 'p28_non-tech_submitted',\n",
    " 'p29_tech_coding_attempt',\n",
    " 'p29_tech_coding_submitted',\n",
    " 'p29_tech_DSA_attempt',\n",
    " 'p29_tech_DSA_submitted',\n",
    " 'p29_non-tech_attempt',\n",
    " 'p29_non-tech_submitted',\n",
    " 'p30_tech_coding_attempt',\n",
    " 'p30_tech_coding_submitted',\n",
    " 'p30_tech_DSA_attempt',\n",
    " 'p30_tech_DSA_submitted',\n",
    " 'p30_non-tech_attempt',\n",
    " 'p30_non-tech_submitted',\n",
    " 'p31_tech_coding_attempt',\n",
    " 'p31_tech_coding_submitted',\n",
    " 'p31_tech_DSA_attempt',\n",
    " 'p31_tech_DSA_submitted',\n",
    " 'p31_non-tech_attempt',\n",
    " 'p31_non-tech_submitted',\n",
    " 'p1_tech_coding_ev',\n",
    " 'p1_tech_DSA_ev',\n",
    " 'p1_non-tech_ev',\n",
    " 'p2_tech_coding_ev',\n",
    " 'p2_tech_DSA_ev',\n",
    " 'p2_non-tech_ev',\n",
    " 'p3_tech_coding_ev',\n",
    " 'p3_tech_DSA_ev',\n",
    " 'p3_non-tech_ev',\n",
    " 'p4_tech_coding_ev',\n",
    " 'p4_tech_DSA_ev',\n",
    " 'p4_non-tech_ev',\n",
    " 'p5_tech_coding_ev',\n",
    " 'p5_tech_DSA_ev',\n",
    " 'p5_non-tech_ev',\n",
    " 'p6_tech_coding_ev',\n",
    " 'p6_tech_DSA_ev',\n",
    " 'p6_non-tech_ev',\n",
    " 'p7_tech_coding_ev',\n",
    " 'p7_tech_DSA_ev',\n",
    " 'p7_non-tech_ev',\n",
    " 'p8_tech_coding_ev',\n",
    " 'p8_tech_DSA_ev',\n",
    " 'p8_non-tech_ev',\n",
    " 'p9_tech_coding_ev',\n",
    " 'p9_tech_DSA_ev',\n",
    " 'p9_non-tech_ev',\n",
    " 'p10_tech_coding_ev',\n",
    " 'p10_tech_DSA_ev',\n",
    " 'p10_non-tech_ev',\n",
    " 'p11_tech_coding_ev',\n",
    " 'p11_tech_DSA_ev',\n",
    " 'p11_non-tech_ev',\n",
    " 'p12_tech_coding_ev',\n",
    " 'p12_tech_DSA_ev',\n",
    " 'p12_non-tech_ev',\n",
    " 'p13_tech_coding_ev',\n",
    " 'p13_tech_DSA_ev',\n",
    " 'p13_non-tech_ev',\n",
    " 'p14_tech_coding_ev',\n",
    " 'p14_tech_DSA_ev',\n",
    " 'p14_non-tech_ev',\n",
    " 'p15_tech_coding_ev',\n",
    " 'p15_tech_DSA_ev',\n",
    " 'p15_non-tech_ev',\n",
    " 'p16_tech_coding_ev',\n",
    " 'p16_tech_DSA_ev',\n",
    " 'p16_non-tech_ev',\n",
    " 'p17_tech_coding_ev',\n",
    " 'p17_tech_DSA_ev',\n",
    " 'p17_non-tech_ev',\n",
    " 'p18_tech_coding_ev',\n",
    " 'p18_tech_DSA_ev',\n",
    " 'p18_non-tech_ev',\n",
    " 'p19_tech_coding_ev',\n",
    " 'p19_tech_DSA_ev',\n",
    " 'p19_non-tech_ev',\n",
    " 'p20_tech_coding_ev',\n",
    " 'p20_tech_DSA_ev',\n",
    " 'p20_non-tech_ev',\n",
    " 'p21_tech_coding_ev',\n",
    " 'p21_tech_DSA_ev',\n",
    " 'p21_non-tech_ev',\n",
    " 'p22_tech_coding_ev',\n",
    " 'p22_tech_DSA_ev',\n",
    " 'p22_non-tech_ev',\n",
    " 'p23_tech_coding_ev',\n",
    " 'p23_tech_DSA_ev',\n",
    " 'p23_non-tech_ev',\n",
    " 'p24_tech_coding_ev',\n",
    " 'p24_tech_DSA_ev',\n",
    " 'p24_non-tech_ev',\n",
    " 'p25_tech_coding_ev',\n",
    " 'p25_tech_DSA_ev',\n",
    " 'p25_non-tech_ev',\n",
    " 'p26_tech_coding_ev',\n",
    " 'p26_tech_DSA_ev',\n",
    " 'p26_non-tech_ev',\n",
    " 'p27_tech_coding_ev',\n",
    " 'p27_tech_DSA_ev',\n",
    " 'p27_non-tech_ev',\n",
    " 'p28_tech_coding_ev',\n",
    " 'p28_tech_DSA_ev',\n",
    " 'p28_non-tech_ev',\n",
    " 'p29_tech_coding_ev',\n",
    " 'p29_tech_DSA_ev',\n",
    " 'p29_non-tech_ev',\n",
    " 'p30_tech_coding_ev',\n",
    " 'p30_tech_DSA_ev',\n",
    " 'p30_non-tech_ev',\n",
    " 'p31_tech_coding_ev',\n",
    " 'p31_tech_DSA_ev',\n",
    " 'p31_non-tech_ev',\n",
    " 'p32_tech_coding_ev',\n",
    " 'p32_tech_DSA_ev',\n",
    " 'p32_non-tech_ev',\n",
    " 'hukumu_score',\n",
    " 'highest_qualification_grouped',\n",
    " 'qualification_specialisation_grouped',\n",
    " 'cleaned_state',\n",
    " 'state_grouped']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_career_level_bands(df):  \n",
    "    conditions = [  \n",
    "        (df['work_experience_duration'] < 2),  \n",
    "        (df['work_experience_duration'] >= 2) & (df['work_experience_duration'] < 5),  \n",
    "        (df['work_experience_duration'] >= 5) & (df['work_experience_duration'] < 8),  \n",
    "        (df['work_experience_duration'] >= 8)  \n",
    "    ]  \n",
    "    choices = ['Entry Level', 'Mid Level', 'Senior Level', 'Expert Level']  \n",
    "    return pd.Series(np.select(conditions, choices, default='Unknown'), index=df.index)  \n",
    "\n",
    "\n",
    "def engineer_features(df):  \n",
    "    # 1. Basic Feature Engineering  \n",
    "    df['experience_squared'] = df['work_experience_duration'] ** 2  \n",
    "    df['experience_flag'] = (df['work_experience_duration'] > 0).astype(int)  \n",
    "    \n",
    "    # 2. Education Score  \n",
    "    education_weights = {  \n",
    "        'B.Tech': 0.8,  \n",
    "        'M.Tech': 1.0,  \n",
    "        'B.Sc': 0.7,  \n",
    "        'M.Sc': 0.9,  \n",
    "        'BCA': 0.6,  \n",
    "        'MCA': 0.8,  \n",
    "        'Others': 0.5  \n",
    "    }  \n",
    "    if 'highest_qualification_grouped' in df.columns:  \n",
    "        df['education_score'] = df['highest_qualification_grouped'].map(education_weights).fillna(0.5)  \n",
    "    else:  \n",
    "        print(\"Warning: 'highest_qualification_grouped' column not found. Using default education score.\")  \n",
    "        df['education_score'] = 0.5  \n",
    "    df['education_score'] = df['highest_qualification_grouped'].map(education_weights).fillna(0.5)  \n",
    "    \n",
    "    # 3. Performance Scores  \n",
    "    def calculate_weighted_score(df, columns):  \n",
    "        \"\"\"  \n",
    "        Calculate the average score across specified columns,   \n",
    "        ignoring NA values.  \n",
    "        \n",
    "        Args:  \n",
    "            df (pd.DataFrame): Input DataFrame  \n",
    "            columns (list): Columns to calculate average from  \n",
    "        \n",
    "        Returns:  \n",
    "            pd.Series: Average scores for each row  \n",
    "        \"\"\"  \n",
    "        # Check if any of the specified columns exist  \n",
    "        if not any(col in df.columns for col in columns):  \n",
    "            return pd.Series(0, index=df.index)  \n",
    "        \n",
    "        # Select existing columns  \n",
    "        existing_cols = [col for col in columns if col in df.columns]  \n",
    "        \n",
    "        # Calculate row-wise mean, ignoring NA values  \n",
    "        return df[existing_cols].mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Define column groups  \n",
    "    coding_cols = [col for col in df.columns if '_tech_coding' in col and '_ev' in col]  \n",
    "    dsa_cols = [col for col in df.columns if '_tech_DSA' in col and '_ev' in col]  \n",
    "    non_tech_cols = [col for col in df.columns if '_non-tech' in col and '_ev' in col]  \n",
    "    \n",
    "    df['coding_score'] = calculate_weighted_score(df, coding_cols)  \n",
    "    df['dsa_score'] = calculate_weighted_score(df, dsa_cols)  \n",
    "    df['non_tech_score'] = calculate_weighted_score(df, non_tech_cols)  \n",
    "    \n",
    "    # 4. Combined Scores  \n",
    "    df['tech_score'] = df['coding_score'] * 0.4 + df['dsa_score'] * 0.6  \n",
    "    df['overall_score'] = df['tech_score'] * 0.8 + df['non_tech_score'] * 0.2  \n",
    "    \n",
    "    # 5. Career Level  \n",
    "    df['career_level'] = create_career_level_bands(df)  \n",
    "    \n",
    "    # Define the top tech stacks to keep\n",
    "\n",
    "    # Clean and standardize tech stack names  \n",
    "    def clean_tech_stack(tech):  \n",
    "        # Convert to uppercase and strip whitespace  \n",
    "        cleaned = str(tech).upper().strip()  \n",
    "        \n",
    "        # Standardization mappings  \n",
    "        replacements = {  \n",
    "            'REACT NATIVE': 'REACT NATIVE',  \n",
    "            'ANDROID': 'ANDROID',  \n",
    "            'MERN+JAVA': 'MERN + JAVA',  \n",
    "            'MERN+ JAVA': 'MERN + JAVA',  \n",
    "            'JAVAIT': 'JAVA IT'  \n",
    "        }  \n",
    "        \n",
    "        return replacements.get(cleaned, cleaned)  \n",
    "\n",
    "    # Clean tech stack names  \n",
    "    df['cleaned_tech_stack'] = df['tech_stack'].apply(clean_tech_stack)  \n",
    "\n",
    "    top_tech_stacks = ['MERN', 'JAVA', 'DA', 'NODE', 'SDET']\n",
    "\n",
    "    # Create a new column for grouped tech stacks\n",
    "    def group_tech_stacks(tech):\n",
    "        return tech if tech in top_tech_stacks else 'OTHERS'\n",
    "\n",
    "    # Apply the grouping\n",
    "    df['tech_stack_grouped_top5'] = df['cleaned_tech_stack'].apply(group_tech_stacks)\n",
    "    \n",
    "    # 7. Location Features  \n",
    "    location_stats = df.groupby('state_grouped').agg({  \n",
    "        'work_experience_duration': ['mean', 'count']  \n",
    "    }).reset_index()  \n",
    "    location_stats.columns = ['state_grouped', 'location_avg_exp', 'location_count']  \n",
    "    df = df.merge(location_stats, on='state_grouped', how='left')  \n",
    "    \n",
    "    # 8. Progress Features  \n",
    "    attendance_cols = [col for col in df.columns if any(x in col for x in ['_tech_DSA', '_tech_coding', '_non-tech'])  \n",
    "                      and not any(x in col for x in ['_attempt', '_submitted', '_ev'])]  \n",
    "    \n",
    "\n",
    "    # 9. Attendance Features  \n",
    "    def count_non_nan_columns(df, column_filter):  \n",
    "        \"\"\"  \n",
    "        Count non-NaN columns for a specific filter  \n",
    "        \"\"\"  \n",
    "        cols = [col for col in attendance_cols if column_filter in col]  \n",
    "        return len([col for col in cols if df[col].notna().any()])  \n",
    "\n",
    "    df['dsa_attendance_rate'] = df[[col for col in attendance_cols if '_tech_DSA' in col]].notna().sum(axis=1) / \\\n",
    "        count_non_nan_columns(df, '_tech_DSA')  \n",
    "\n",
    "    df['coding_attendance_rate'] = df[[col for col in attendance_cols if '_tech_coding' in col]].notna().sum(axis=1) / \\\n",
    "        count_non_nan_columns(df, '_tech_coding')  \n",
    "\n",
    "    df['nontech_attendance_rate'] = df[[col for col in attendance_cols if '_non-tech' in col]].notna().sum(axis=1) / \\\n",
    "        count_non_nan_columns(df, '_non-tech')  \n",
    "\n",
    "    df['overall_attendance_rate'] = df[attendance_cols].notna().sum(axis=1) / \\\n",
    "        count_non_nan_columns(df, '')  \n",
    "\n",
    "    # 10. Attempt Features  \n",
    "    attempt_cols = [col for col in df.columns if '_attempt' in col]  \n",
    "    dsa_attempt_cols = [col for col in attempt_cols if '_tech_DSA' in col]  \n",
    "    coding_attempt_cols = [col for col in attempt_cols if '_tech_coding' in col]  \n",
    "    nontech_attempt_cols = [col for col in attempt_cols if '_non-tech' in col]  \n",
    "\n",
    "    # Total attempts considering only non-NaN values  \n",
    "    df['total_attempts'] = df[attempt_cols].apply(pd.Series.dropna, axis=1).sum(axis=1)  \n",
    "    df['avg_attempts_per_test'] = df[attempt_cols].apply(pd.Series.dropna, axis=1).mean(axis=1)  \n",
    "    df['max_attempts'] = df[attempt_cols].apply(pd.Series.dropna, axis=1).max(axis=1)  \n",
    "\n",
    "\n",
    "    # 11. Submission Features  \n",
    "    submit_cols = [col for col in df.columns if '_submitted' in col]  \n",
    "    dsa_submit_cols = [col for col in submit_cols if '_tech_DSA' in col]  \n",
    "    coding_submit_cols = [col for col in submit_cols if '_tech_coding' in col]  \n",
    "    nontech_submit_cols = [col for col in submit_cols if '_non-tech' in col]  \n",
    "\n",
    "    # Submission rates considering only non-NaN values  \n",
    "    df['submission_rate'] = df[submit_cols].apply(pd.Series.dropna, axis=1).mean(axis=1)  \n",
    "    df['dsa_submission_rate'] = df[dsa_submit_cols].apply(pd.Series.dropna, axis=1).mean(axis=1)  \n",
    "    df['coding_submission_rate'] = df[coding_submit_cols].apply(pd.Series.dropna, axis=1).mean(axis=1)  \n",
    "    df['nontech_submission_rate'] = df[nontech_submit_cols].apply(pd.Series.dropna, axis=1).mean(axis=1)\n",
    "        \n",
    "    # 9. Attendance Features  \n",
    "    # DSA Attendance  \n",
    "    dsa_attendance_columns = ['p1_tech_DSA', 'p2_tech_DSA', 'p3_tech_DSA', 'p4_tech_DSA', 'p5_tech_DSA']  \n",
    "    df['avg_dsa_attendance'] = df[dsa_attendance_columns].mean(axis=1, skipna=True)  \n",
    "\n",
    "    # Coding Attendance  \n",
    "    coding_attendance_columns = ['p1_tech_coding', 'p2_tech_coding', 'p3_tech_coding', 'p4_tech_coding', 'p5_tech_coding']  \n",
    "    df['avg_coding_attendance'] = df[coding_attendance_columns].mean(axis=1, skipna=True)  \n",
    "\n",
    "    # Non-Tech Attendance  \n",
    "    nontech_attendance_columns = ['p1_non-tech', 'p2_non-tech', 'p3_non-tech', 'p4_non-tech', 'p5_non-tech']  \n",
    "    df['avg_nontech_attendance'] = df[nontech_attendance_columns].mean(axis=1, skipna=True)  \n",
    "\n",
    "    # 10. Attempt Features  \n",
    "    # DSA Attempts  \n",
    "    dsa_attempt_columns = ['p1_tech_DSA_attempt', 'p2_tech_DSA_attempt', 'p3_tech_DSA_attempt', 'p4_tech_DSA_attempt', 'p5_tech_DSA_attempt']  \n",
    "    df['avg_dsa_attempt'] = df[dsa_attempt_columns].mean(axis=1, skipna=True)  \n",
    "\n",
    "    # Coding Attempts  \n",
    "    coding_attempt_columns = ['p1_tech_coding_attempt', 'p2_tech_coding_attempt', 'p3_tech_coding_attempt', 'p4_tech_coding_attempt', 'p5_tech_coding_attempt']  \n",
    "    df['avg_coding_attempt'] = df[coding_attempt_columns].mean(axis=1, skipna=True)  \n",
    "\n",
    "    # Non-Tech Attempts  \n",
    "    nontech_attempt_columns = ['p1_non-tech_attempt', 'p2_non-tech_attempt', 'p3_non-tech_attempt', 'p4_non-tech_attempt', 'p5_non-tech_attempt']  \n",
    "    df['avg_nontech_attempt'] = df[nontech_attempt_columns].mean(axis=1, skipna=True)  \n",
    "\n",
    "\n",
    "    # 11. Submission Features  \n",
    "    # DSA Submissions  \n",
    "    dsa_submission_columns = ['p1_tech_DSA_submitted', 'p2_tech_DSA_submitted', 'p3_tech_DSA_submitted', 'p4_tech_DSA_submitted', 'p5_tech_DSA_submitted']  \n",
    "    df['avg_dsa_submission'] = df[dsa_submission_columns].mean(axis=1, skipna=True)  \n",
    "\n",
    "    # Coding Submissions  \n",
    "    coding_submission_columns = ['p1_tech_coding_submitted', 'p2_tech_coding_submitted', 'p3_tech_coding_submitted', 'p4_tech_coding_submitted', 'p5_tech_coding_submitted']  \n",
    "    df['avg_coding_submission'] = df[coding_submission_columns].mean(axis=1, skipna=True)  \n",
    "\n",
    "    # Non-Tech Submissions  \n",
    "    nontech_submission_columns = ['p1_non-tech_submitted', 'p2_non-tech_submitted', 'p3_non-tech_submitted', 'p4_non-tech_submitted', 'p5_non-tech_submitted']  \n",
    "    df['avg_nontech_submission'] = df[nontech_submission_columns].mean(axis=1, skipna=True)  \n",
    "\n",
    "    \n",
    "\n",
    "    # 13. Interaction Features  \n",
    "    df['exp_edu_interaction'] = df['education_score'] * (1 + df['work_experience_duration'])  \n",
    "    df['tech_exp_interaction'] = df['tech_score'] * (1 + df['work_experience_duration'])  \n",
    "\n",
    "        #14 Buckets\n",
    "    bins = [0, 30, 60, float('inf')]  \n",
    "    labels = ['0 to 30', '30 to 60', '60+'] \n",
    "\n",
    "\n",
    "    metrics = [  \n",
    "    'avg_dsa_attendance',  \n",
    "    'avg_coding_attendance',  \n",
    "    'avg_nontech_attendance',  \n",
    "    'avg_dsa_attempt',  \n",
    "    'avg_coding_attempt',  \n",
    "    'avg_nontech_attempt',  \n",
    "    'avg_dsa_submission',  \n",
    "    'avg_coding_submission',  \n",
    "    'avg_nontech_submission'  \n",
    "    ]        \n",
    "\n",
    "    # Create bucket columns  \n",
    "    for metric in metrics:  \n",
    "        df[f'{metric}_bucket'] = pd.cut(df[metric], bins=bins, labels=labels, right=False)  \n",
    "\n",
    "    \n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[  \n",
    "#     'coding_score',  \n",
    "#     'dsa_score',  \n",
    "#     'non_tech_score',  \n",
    "#     'tech_score',  \n",
    "#     'overall_score',  \n",
    "#     'dsa_attendance_rate',  \n",
    "#     'coding_attendance_rate',  \n",
    "#     'nontech_attendance_rate',  \n",
    "#     'overall_attendance_rate',  \n",
    "#     'total_attempts',  \n",
    "#     'avg_attempts_per_test',  \n",
    "#     'max_attempts',  \n",
    "#     'submission_rate',  \n",
    "#     'dsa_submission_rate',  \n",
    "#     'coding_submission_rate',  \n",
    "#     'nontech_submission_rate',  \n",
    "#     'exp_edu_interaction',  \n",
    "#     'tech_exp_interaction',  \n",
    "#     'tech_stack_grouped_top5',  \n",
    "#     'highest_qualification_grouped',  \n",
    "#     'gender',  \n",
    "#     'state_grouped',  \n",
    "#     'experience_flag',  \n",
    "#     'career_level',  \n",
    "#     'work_experience_any',  \n",
    "#     'avg_dsa_attendance_bucket',  \n",
    "#     'avg_coding_attendance_bucket',  \n",
    "#     'avg_nontech_attendance_bucket',  \n",
    "#     'avg_dsa_attempt_bucket',  \n",
    "#     'avg_coding_attempt_bucket',  \n",
    "#     'avg_nontech_attempt_bucket',  \n",
    "#     'avg_dsa_submission_bucket',  \n",
    "#     'avg_coding_submission_bucket',  \n",
    "#     'avg_nontech_submission_bucket'  \n",
    "# ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Summary:\n",
      "Total predictions: 178\n",
      "\n",
      "Predicted CTC Statistics:\n",
      "count       178.000000\n",
      "mean     563741.409913\n",
      "std       20844.944792\n",
      "min      507012.748718\n",
      "25%      551575.767994\n",
      "50%      561805.176735\n",
      "75%      571512.997150\n",
      "max      636297.035217\n",
      "dtype: float64\n",
      "\n",
      "Prediction Error Summary:\n",
      "       Absolute Error  Percentage Error\n",
      "count             0.0               0.0\n",
      "mean              NaN               NaN\n",
      "std               NaN               NaN\n",
      "min               NaN               NaN\n",
      "25%               NaN               NaN\n",
      "50%               NaN               NaN\n",
      "75%               NaN               NaN\n",
      "max               NaN               NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import joblib  \n",
    "\n",
    "# Load the saved model artifacts  \n",
    "# Note: Replace these paths with the actual paths to your saved artifacts  \n",
    "model_path = 'salary_model_20250220_133138.joblib'  \n",
    "encoders_path = 'salary_model_encoders_20250220_133138.joblib'  \n",
    "features_path = 'salary_model_features_20250220_133138.txt'  \n",
    "\n",
    "# Load the model  \n",
    "model = joblib.load(model_path)  \n",
    "\n",
    "# Load the encoders  \n",
    "encoders = joblib.load(encoders_path)  \n",
    "\n",
    "# Load the feature list  \n",
    "with open(features_path, 'r') as f:  \n",
    "    feature_list = f.read().splitlines()  \n",
    "\n",
    "def predict_salary(model, new_data, encoders, feature_list):  \n",
    "    \"\"\"  \n",
    "    Make predictions for new candidates  \n",
    "    \"\"\"  \n",
    "    # Prepare the data  \n",
    "    processed_data = new_data.copy()  \n",
    "    \n",
    "    # Apply feature engineering (use the same function from the previous script)  \n",
    "    processed_data = engineer_features(processed_data)  \n",
    "    \n",
    "    # Apply encoders  \n",
    "    for feature, encoder in encoders.items():  \n",
    "        if feature in processed_data.columns:  \n",
    "            processed_data[feature] = processed_data[feature].astype(str)  \n",
    "            processed_data[feature] = processed_data[feature].replace('nan', 'Unknown')  \n",
    "            processed_data[feature] = encoder.transform(processed_data[feature])  \n",
    "    \n",
    "    # Ensure all features are present  \n",
    "    for feature in feature_list:  \n",
    "        if feature not in processed_data.columns:  \n",
    "            processed_data[feature] = 0  \n",
    "    \n",
    "    # Reorder columns to match training data  \n",
    "    processed_data = processed_data[feature_list]  \n",
    "    \n",
    "    # Make predictions and convert back to original scale  \n",
    "    predictions = model.predict(processed_data) * 100000  \n",
    "    \n",
    "    return predictions  \n",
    "\n",
    "# Make predictions  \n",
    "predictions = predict_salary(model, df, encoders, feature_list)  \n",
    "\n",
    "# Add predictions to the dataframe  \n",
    "df['predicted_ctc'] = predictions  \n",
    "\n",
    "# Optional: Save predictions  \n",
    "df.to_csv('predictions_output.csv', index=False)  \n",
    "\n",
    "# Print some summary statistics  \n",
    "print(\"Prediction Summary:\")  \n",
    "print(f\"Total predictions: {len(predictions)}\")  \n",
    "print(\"\\nPredicted CTC Statistics:\")  \n",
    "print(pd.Series(predictions).describe())  \n",
    "\n",
    "# Optional: Visualize prediction distribution  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.hist(predictions, bins=30, edgecolor='black')  \n",
    "plt.title('Distribution of Predicted CTC')  \n",
    "plt.xlabel('Predicted CTC (INR)')  \n",
    "plt.ylabel('Frequency')  \n",
    "plt.savefig('predictions_distribution.png')  \n",
    "plt.close()  \n",
    "\n",
    "# Optional: Create a comparison DataFrame if actual CTC exists  \n",
    "if 'ctc' in df.columns:  \n",
    "    comparison_df = pd.DataFrame({  \n",
    "        'Actual CTC': df['ctc'] * 100000,  \n",
    "        'Predicted CTC': predictions,  \n",
    "        'Absolute Error': np.abs(df['ctc'] * 100000 - predictions),  \n",
    "        'Percentage Error': np.abs((df['ctc'] * 100000 - predictions) / (df['ctc'] * 100000)) * 100  \n",
    "    })  \n",
    "    \n",
    "    print(\"\\nPrediction Error Summary:\")  \n",
    "    print(comparison_df[['Absolute Error', 'Percentage Error']].describe())  \n",
    "    \n",
    "    # Save comparison  \n",
    "    comparison_df.to_csv('ctc_prediction_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'user_id',\n",
       " 'section_id',\n",
       " 'date_of_clearance',\n",
       " 'p11_tech_coding',\n",
       " 'p11_tech_DSA',\n",
       " 'p11_non-tech',\n",
       " 'p12_tech_coding',\n",
       " 'p12_tech_DSA',\n",
       " 'p12_non-tech',\n",
       " 'p13_tech_coding',\n",
       " 'p13_tech_DSA',\n",
       " 'p13_non-tech',\n",
       " 'p14_tech_coding',\n",
       " 'p14_tech_DSA',\n",
       " 'p14_non-tech',\n",
       " 'p15_tech_coding',\n",
       " 'p15_tech_DSA',\n",
       " 'p15_non-tech',\n",
       " 'p16_tech_coding',\n",
       " 'p16_tech_DSA',\n",
       " 'p16_non-tech',\n",
       " 'p17_tech_coding',\n",
       " 'p17_tech_DSA',\n",
       " 'p17_non-tech',\n",
       " 'p18_tech_coding',\n",
       " 'p18_tech_DSA',\n",
       " 'p18_non-tech',\n",
       " 'p19_tech_coding',\n",
       " 'p19_tech_DSA',\n",
       " 'p19_non-tech',\n",
       " 'p20_tech_coding',\n",
       " 'p20_tech_DSA',\n",
       " 'p20_non-tech',\n",
       " 'p21_tech_coding',\n",
       " 'p21_tech_DSA',\n",
       " 'p21_non-tech',\n",
       " 'p22_tech_coding',\n",
       " 'p22_tech_DSA',\n",
       " 'p22_non-tech',\n",
       " 'p23_tech_coding',\n",
       " 'p23_tech_DSA',\n",
       " 'p23_non-tech',\n",
       " 'p24_tech_coding',\n",
       " 'p24_tech_DSA',\n",
       " 'p24_non-tech',\n",
       " 'p25_tech_coding',\n",
       " 'p25_tech_DSA',\n",
       " 'p25_non-tech',\n",
       " 'p26_tech_coding',\n",
       " 'p26_tech_DSA',\n",
       " 'p26_non-tech',\n",
       " 'p27_tech_coding',\n",
       " 'p27_tech_DSA',\n",
       " 'p27_non-tech',\n",
       " 'p28_tech_coding',\n",
       " 'p28_tech_DSA',\n",
       " 'p28_non-tech',\n",
       " 'p29_tech_coding',\n",
       " 'p29_tech_DSA',\n",
       " 'p29_non-tech',\n",
       " 'p30_tech_coding',\n",
       " 'p30_tech_DSA',\n",
       " 'p30_non-tech',\n",
       " 'p31_tech_coding',\n",
       " 'p31_tech_DSA',\n",
       " 'p31_non-tech',\n",
       " 'p32_tech_coding',\n",
       " 'p32_tech_DSA',\n",
       " 'p32_non-tech',\n",
       " 'p1_tech_coding_attempt',\n",
       " 'p1_tech_coding_submitted',\n",
       " 'p1_tech_DSA_attempt',\n",
       " 'p1_tech_DSA_submitted',\n",
       " 'p1_non-tech_attempt',\n",
       " 'p1_non-tech_submitted',\n",
       " 'p2_tech_coding_attempt',\n",
       " 'p2_tech_coding_submitted',\n",
       " 'p2_tech_DSA_attempt',\n",
       " 'p2_tech_DSA_submitted',\n",
       " 'p2_non-tech_attempt',\n",
       " 'p2_non-tech_submitted',\n",
       " 'p3_tech_coding_attempt',\n",
       " 'p3_tech_coding_submitted',\n",
       " 'p3_tech_DSA_attempt',\n",
       " 'p3_tech_DSA_submitted',\n",
       " 'p3_non-tech_attempt',\n",
       " 'p3_non-tech_submitted',\n",
       " 'p4_tech_coding_attempt',\n",
       " 'p4_tech_coding_submitted',\n",
       " 'p4_tech_DSA_attempt',\n",
       " 'p4_tech_DSA_submitted',\n",
       " 'p4_non-tech_attempt',\n",
       " 'p4_non-tech_submitted',\n",
       " 'p5_tech_coding_attempt',\n",
       " 'p5_tech_coding_submitted',\n",
       " 'p5_tech_DSA_attempt',\n",
       " 'p5_tech_DSA_submitted',\n",
       " 'p5_non-tech_attempt',\n",
       " 'p5_non-tech_submitted',\n",
       " 'p6_tech_coding_attempt',\n",
       " 'p6_tech_coding_submitted',\n",
       " 'p6_tech_DSA_attempt',\n",
       " 'p6_tech_DSA_submitted',\n",
       " 'p6_non-tech_attempt',\n",
       " 'p6_non-tech_submitted',\n",
       " 'p7_tech_coding_attempt',\n",
       " 'p7_tech_coding_submitted',\n",
       " 'p7_tech_DSA_attempt',\n",
       " 'p7_tech_DSA_submitted',\n",
       " 'p7_non-tech_attempt',\n",
       " 'p7_non-tech_submitted',\n",
       " 'p8_tech_coding_attempt',\n",
       " 'p8_tech_coding_submitted',\n",
       " 'p8_tech_DSA_attempt',\n",
       " 'p8_tech_DSA_submitted',\n",
       " 'p8_non-tech_attempt',\n",
       " 'p8_non-tech_submitted',\n",
       " 'p9_tech_coding_attempt',\n",
       " 'p9_tech_coding_submitted',\n",
       " 'p9_tech_DSA_attempt',\n",
       " 'p9_tech_DSA_submitted',\n",
       " 'p9_non-tech_attempt',\n",
       " 'p9_non-tech_submitted',\n",
       " 'p10_tech_coding_attempt',\n",
       " 'p10_tech_coding_submitted',\n",
       " 'p10_tech_DSA_attempt',\n",
       " 'p10_tech_DSA_submitted',\n",
       " 'p10_non-tech_attempt',\n",
       " 'p10_non-tech_submitted',\n",
       " 'p11_tech_coding_attempt',\n",
       " 'p11_tech_coding_submitted',\n",
       " 'p11_tech_DSA_attempt',\n",
       " 'p11_tech_DSA_submitted',\n",
       " 'p11_non-tech_attempt',\n",
       " 'p11_non-tech_submitted',\n",
       " 'p12_tech_coding_attempt',\n",
       " 'p12_tech_coding_submitted',\n",
       " 'p12_tech_DSA_attempt',\n",
       " 'p12_tech_DSA_submitted',\n",
       " 'p12_non-tech_attempt',\n",
       " 'p12_non-tech_submitted',\n",
       " 'p13_tech_coding_attempt',\n",
       " 'p13_tech_coding_submitted',\n",
       " 'p13_tech_DSA_attempt',\n",
       " 'p13_tech_DSA_submitted',\n",
       " 'p13_non-tech_attempt',\n",
       " 'p13_non-tech_submitted',\n",
       " 'p14_tech_coding_attempt',\n",
       " 'p14_tech_coding_submitted',\n",
       " 'p14_tech_DSA_attempt',\n",
       " 'p14_tech_DSA_submitted',\n",
       " 'p14_non-tech_attempt',\n",
       " 'p14_non-tech_submitted',\n",
       " 'p15_tech_coding_attempt',\n",
       " 'p15_tech_coding_submitted',\n",
       " 'p15_tech_DSA_attempt',\n",
       " 'p15_tech_DSA_submitted',\n",
       " 'p15_non-tech_attempt',\n",
       " 'p15_non-tech_submitted',\n",
       " 'p16_tech_coding_attempt',\n",
       " 'p16_tech_coding_submitted',\n",
       " 'p16_tech_DSA_attempt',\n",
       " 'p16_tech_DSA_submitted',\n",
       " 'p16_non-tech_attempt',\n",
       " 'p16_non-tech_submitted',\n",
       " 'p17_tech_coding_attempt',\n",
       " 'p17_tech_coding_submitted',\n",
       " 'p17_tech_DSA_attempt',\n",
       " 'p17_tech_DSA_submitted',\n",
       " 'p17_non-tech_attempt',\n",
       " 'p17_non-tech_submitted',\n",
       " 'p18_tech_coding_attempt',\n",
       " 'p18_tech_coding_submitted',\n",
       " 'p18_tech_DSA_attempt',\n",
       " 'p18_tech_DSA_submitted',\n",
       " 'p18_non-tech_attempt',\n",
       " 'p18_non-tech_submitted',\n",
       " 'p19_tech_coding_attempt',\n",
       " 'p19_tech_coding_submitted',\n",
       " 'p19_tech_DSA_attempt',\n",
       " 'p19_tech_DSA_submitted',\n",
       " 'p19_non-tech_attempt',\n",
       " 'p19_non-tech_submitted',\n",
       " 'p20_tech_coding_attempt',\n",
       " 'p20_tech_coding_submitted',\n",
       " 'p20_tech_DSA_attempt',\n",
       " 'p20_tech_DSA_submitted',\n",
       " 'p20_non-tech_attempt',\n",
       " 'p20_non-tech_submitted',\n",
       " 'p21_tech_coding_attempt',\n",
       " 'p21_tech_coding_submitted',\n",
       " 'p21_tech_DSA_attempt',\n",
       " 'p21_tech_DSA_submitted',\n",
       " 'p21_non-tech_attempt',\n",
       " 'p21_non-tech_submitted',\n",
       " 'p22_tech_coding_attempt',\n",
       " 'p22_tech_coding_submitted',\n",
       " 'p22_tech_DSA_attempt',\n",
       " 'p22_tech_DSA_submitted',\n",
       " 'p22_non-tech_attempt',\n",
       " 'p22_non-tech_submitted',\n",
       " 'p23_tech_coding_attempt',\n",
       " 'p23_tech_coding_submitted',\n",
       " 'p23_tech_DSA_attempt',\n",
       " 'p23_tech_DSA_submitted',\n",
       " 'p23_non-tech_attempt',\n",
       " 'p23_non-tech_submitted',\n",
       " 'p24_tech_coding_attempt',\n",
       " 'p24_tech_coding_submitted',\n",
       " 'p24_tech_DSA_attempt',\n",
       " 'p24_tech_DSA_submitted',\n",
       " 'p24_non-tech_attempt',\n",
       " 'p24_non-tech_submitted',\n",
       " 'p25_tech_coding_attempt',\n",
       " 'p25_tech_coding_submitted',\n",
       " 'p25_tech_DSA_attempt',\n",
       " 'p25_tech_DSA_submitted',\n",
       " 'p25_non-tech_attempt',\n",
       " 'p25_non-tech_submitted',\n",
       " 'p26_tech_coding_attempt',\n",
       " 'p26_tech_coding_submitted',\n",
       " 'p26_tech_DSA_attempt',\n",
       " 'p26_tech_DSA_submitted',\n",
       " 'p26_non-tech_attempt',\n",
       " 'p26_non-tech_submitted',\n",
       " 'p27_tech_coding_attempt',\n",
       " 'p27_tech_coding_submitted',\n",
       " 'p27_tech_DSA_attempt',\n",
       " 'p27_tech_DSA_submitted',\n",
       " 'p27_non-tech_attempt',\n",
       " 'p27_non-tech_submitted',\n",
       " 'p28_tech_coding_attempt',\n",
       " 'p28_tech_coding_submitted',\n",
       " 'p28_tech_DSA_attempt',\n",
       " 'p28_tech_DSA_submitted',\n",
       " 'p28_non-tech_attempt',\n",
       " 'p28_non-tech_submitted',\n",
       " 'p29_tech_coding_attempt',\n",
       " 'p29_tech_coding_submitted',\n",
       " 'p29_tech_DSA_attempt',\n",
       " 'p29_tech_DSA_submitted',\n",
       " 'p29_non-tech_attempt',\n",
       " 'p29_non-tech_submitted',\n",
       " 'p30_tech_coding_attempt',\n",
       " 'p30_tech_coding_submitted',\n",
       " 'p30_tech_DSA_attempt',\n",
       " 'p30_tech_DSA_submitted',\n",
       " 'p30_non-tech_attempt',\n",
       " 'p30_non-tech_submitted',\n",
       " 'p31_tech_coding_attempt',\n",
       " 'p31_tech_coding_submitted',\n",
       " 'p31_tech_DSA_attempt',\n",
       " 'p31_tech_DSA_submitted',\n",
       " 'p31_non-tech_attempt',\n",
       " 'p31_non-tech_submitted',\n",
       " 'p1_tech_coding_ev',\n",
       " 'p1_tech_DSA_ev',\n",
       " 'p1_non-tech_ev',\n",
       " 'p2_tech_coding_ev',\n",
       " 'p2_tech_DSA_ev',\n",
       " 'p2_non-tech_ev',\n",
       " 'p3_tech_coding_ev',\n",
       " 'p3_tech_DSA_ev',\n",
       " 'p3_non-tech_ev',\n",
       " 'p4_tech_coding_ev',\n",
       " 'p4_tech_DSA_ev',\n",
       " 'p4_non-tech_ev',\n",
       " 'p5_tech_coding_ev',\n",
       " 'p5_tech_DSA_ev',\n",
       " 'p5_non-tech_ev',\n",
       " 'p6_tech_coding_ev',\n",
       " 'p6_tech_DSA_ev',\n",
       " 'p6_non-tech_ev',\n",
       " 'p7_tech_coding_ev',\n",
       " 'p7_tech_DSA_ev',\n",
       " 'p7_non-tech_ev',\n",
       " 'p8_tech_coding_ev',\n",
       " 'p8_tech_DSA_ev',\n",
       " 'p8_non-tech_ev',\n",
       " 'p9_tech_coding_ev',\n",
       " 'p9_tech_DSA_ev',\n",
       " 'p9_non-tech_ev',\n",
       " 'p10_tech_coding_ev',\n",
       " 'p10_tech_DSA_ev',\n",
       " 'p10_non-tech_ev',\n",
       " 'p11_tech_coding_ev',\n",
       " 'p11_tech_DSA_ev',\n",
       " 'p11_non-tech_ev',\n",
       " 'p12_tech_coding_ev',\n",
       " 'p12_tech_DSA_ev',\n",
       " 'p12_non-tech_ev',\n",
       " 'p13_tech_coding_ev',\n",
       " 'p13_tech_DSA_ev',\n",
       " 'p13_non-tech_ev',\n",
       " 'p14_tech_coding_ev',\n",
       " 'p14_tech_DSA_ev',\n",
       " 'p14_non-tech_ev',\n",
       " 'p15_tech_coding_ev',\n",
       " 'p15_tech_DSA_ev',\n",
       " 'p15_non-tech_ev',\n",
       " 'p16_tech_coding_ev',\n",
       " 'p16_tech_DSA_ev',\n",
       " 'p16_non-tech_ev',\n",
       " 'p17_tech_coding_ev',\n",
       " 'p17_tech_DSA_ev',\n",
       " 'p17_non-tech_ev',\n",
       " 'p18_tech_coding_ev',\n",
       " 'p18_tech_DSA_ev',\n",
       " 'p18_non-tech_ev',\n",
       " 'p19_tech_coding_ev',\n",
       " 'p19_tech_DSA_ev',\n",
       " 'p19_non-tech_ev',\n",
       " 'p20_tech_coding_ev',\n",
       " 'p20_tech_DSA_ev',\n",
       " 'p20_non-tech_ev',\n",
       " 'p21_tech_coding_ev',\n",
       " 'p21_tech_DSA_ev',\n",
       " 'p21_non-tech_ev',\n",
       " 'p22_tech_coding_ev',\n",
       " 'p22_tech_DSA_ev',\n",
       " 'p22_non-tech_ev',\n",
       " 'p23_tech_coding_ev',\n",
       " 'p23_tech_DSA_ev',\n",
       " 'p23_non-tech_ev',\n",
       " 'p24_tech_coding_ev',\n",
       " 'p24_tech_DSA_ev',\n",
       " 'p24_non-tech_ev',\n",
       " 'p25_tech_coding_ev',\n",
       " 'p25_tech_DSA_ev',\n",
       " 'p25_non-tech_ev',\n",
       " 'p26_tech_coding_ev',\n",
       " 'p26_tech_DSA_ev',\n",
       " 'p26_non-tech_ev',\n",
       " 'p27_tech_coding_ev',\n",
       " 'p27_tech_DSA_ev',\n",
       " 'p27_non-tech_ev',\n",
       " 'p28_tech_coding_ev',\n",
       " 'p28_tech_DSA_ev',\n",
       " 'p28_non-tech_ev',\n",
       " 'p29_tech_coding_ev',\n",
       " 'p29_tech_DSA_ev',\n",
       " 'p29_non-tech_ev',\n",
       " 'p30_tech_coding_ev',\n",
       " 'p30_tech_DSA_ev',\n",
       " 'p30_non-tech_ev',\n",
       " 'p31_tech_coding_ev',\n",
       " 'p31_tech_DSA_ev',\n",
       " 'p31_non-tech_ev',\n",
       " 'p32_tech_coding_ev',\n",
       " 'p32_tech_DSA_ev',\n",
       " 'p32_non-tech_ev',\n",
       " 'hukumu_score']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_csv('predictions_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['predicted_ctc_normalized'] = out['predicted_ctc'] / 100000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out[['predicted_ctc_normalized','user_code','user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('r1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
